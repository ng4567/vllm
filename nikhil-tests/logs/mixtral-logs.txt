Generated 20 configs:
  - mistral-7b-instruct-v0.1_large_shared_arc
  - mistral-7b-instruct-v0.1_large_shared_lru
  - mistral-7b-instruct-v0.1_large_uniq_arc
  - mistral-7b-instruct-v0.1_large_uniq_lru
  - mistral-7b-instruct-v0.1_medium_shared_arc
  - mistral-7b-instruct-v0.1_medium_shared_lru
  - mistral-7b-instruct-v0.1_medium_uniq_arc
  - mistral-7b-instruct-v0.1_medium_uniq_lru
  - mistral-7b-instruct-v0.1_small_shared_arc
  - mistral-7b-instruct-v0.1_small_shared_lru
  - mistral-7b-instruct-v0.1_small_uniq_arc
  - mistral-7b-instruct-v0.1_small_uniq_lru
  - mistral-7b-instruct-v0.1_xlarge_shared_arc
  - mistral-7b-instruct-v0.1_xlarge_shared_lru
  - mistral-7b-instruct-v0.1_xlarge_uniq_arc
  - mistral-7b-instruct-v0.1_xlarge_uniq_lru
  - mistral-7b-instruct-v0.1_xsmall_shared_arc
  - mistral-7b-instruct-v0.1_xsmall_shared_lru
  - mistral-7b-instruct-v0.1_xsmall_uniq_arc
  - mistral-7b-instruct-v0.1_xsmall_uniq_lru

Initializing Config: mistral-7b-instruct-v0.1_xsmall_uniq_lru

=== Max Offload Blocks Calculation ===
Model: mistralai/Mistral-7B-Instruct-v0.1
  Layers: 32, KV Heads: 8, Head Dim: 128
  Block size: 16 tokens
  Bytes per block: 2048.0 KB
Destination GPU 1:
  Total: 100.0 GB
  Free: 99.4 GB
  Usable (with 10% headroom): 89.5 GB
  Max offload blocks: 42,657
======================================

Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 40.01 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xsmall_uniq_lru: 95.02 GB

Initializing Config: mistral-7b-instruct-v0.1_xsmall_uniq_arc
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 40.01 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xsmall_uniq_arc: 95.02 GB

Initializing Config: mistral-7b-instruct-v0.1_small_uniq_lru
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 80.02 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_small_uniq_lru: 175.04 GB

Initializing Config: mistral-7b-instruct-v0.1_small_uniq_arc
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 80.02 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_small_uniq_arc: 175.04 GB

Initializing Config: mistral-7b-instruct-v0.1_medium_uniq_lru
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 160.04 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_medium_uniq_lru: 335.08 GB

Initializing Config: mistral-7b-instruct-v0.1_medium_uniq_arc
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 160.04 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_medium_uniq_arc: 335.08 GB

Initializing Config: mistral-7b-instruct-v0.1_large_uniq_lru
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 240.06 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_large_uniq_lru: 495.12 GB

Initializing Config: mistral-7b-instruct-v0.1_large_uniq_arc
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 240.06 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_large_uniq_arc: 495.12 GB

Initializing Config: mistral-7b-instruct-v0.1_xlarge_uniq_lru
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 400.10 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xlarge_uniq_lru: 815.19 GB

Initializing Config: mistral-7b-instruct-v0.1_xlarge_uniq_arc
Loaded 1000 prompts from 'unique'
Average input length: 105 tokens (from 1000 prompts)
Full KV cache would need: 400.10 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xlarge_uniq_arc: 815.19 GB

Initializing Config: mistral-7b-instruct-v0.1_xsmall_shared_lru
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 39.85 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xsmall_shared_lru: 94.69 GB

Initializing Config: mistral-7b-instruct-v0.1_xsmall_shared_arc
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 39.85 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xsmall_shared_arc: 94.69 GB

Initializing Config: mistral-7b-instruct-v0.1_small_shared_lru
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 79.69 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_small_shared_lru: 174.38 GB

Initializing Config: mistral-7b-instruct-v0.1_small_shared_arc
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 79.69 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_small_shared_arc: 174.38 GB

Initializing Config: mistral-7b-instruct-v0.1_medium_shared_lru
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 159.38 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_medium_shared_lru: 333.77 GB

Initializing Config: mistral-7b-instruct-v0.1_medium_shared_arc
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 159.38 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_medium_shared_arc: 333.77 GB

Initializing Config: mistral-7b-instruct-v0.1_large_shared_lru
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 239.08 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_large_shared_lru: 493.15 GB

Initializing Config: mistral-7b-instruct-v0.1_large_shared_arc
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 239.08 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_large_shared_arc: 493.15 GB

Initializing Config: mistral-7b-instruct-v0.1_xlarge_shared_lru
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 398.46 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xlarge_shared_lru: 811.92 GB

Initializing Config: mistral-7b-instruct-v0.1_xlarge_shared_arc
Loaded 1000 prompts from 'shared_prefix'
Average input length: 80 tokens (from 1000 prompts)
Full KV cache would need: 398.46 GB (will be offloaded)
Memory Usage for Config mistral-7b-instruct-v0.1_xlarge_shared_arc: 811.92 GB


============================================================
Running tests for: mistral-7b-instruct-v0.1_xsmall_uniq_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 20:58:27 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='fe2e744d-8369-4c95-abb8-6eada8dabbac', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
WARNING 12-09 20:58:27 [arg_utils.py:1151] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
INFO 12-09 20:58:34 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 20:58:34 [model.py:1753] Using max model len 32768
INFO 12-09 20:58:34 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 20:58:34 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
WARNING 12-09 20:58:35 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:58:40 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:58:42 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:39423 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:58:42 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:58:42 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:58:55 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:02 [weight_utils.py:487] Time spent downloading weights for mistralai/Mistral-7B-Instruct-v0.1: 6.651257 seconds
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:04 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:05 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 22.100185 seconds
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:09 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:09 [backends.py:715] Dynamo bytecode transform time: 3.80 s
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:13 [backends.py:257] Cache the graph for dynamic shape for later use
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:22 [backends.py:288] Compiling a graph for dynamic shape takes 12.95 s
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:24 [monitor.py:34] torch.compile takes 16.75 s in total
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [gpu_worker.py:348] Available KV cache memory: 12.52 GiB
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [kv_cache_utils.py:1286] GPU KV cache size: 102,528 tokens
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.00x
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: fe2e744d-8369-4c95-abb8-6eada8dabbac
[0;36m(EngineCore_DP0 pid=51619)[0;0m WARNING 12-09 20:59:25 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=51619)[0;0m WARNING 12-09 20:59:25 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:25 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6408, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:26 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:31 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:31 [core.py:253] init engine (profile, create kv cache, warmup model) took 25.93 seconds
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:31 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: fe2e744d-8369-4c95-abb8-6eada8dabbac
[0;36m(EngineCore_DP0 pid=51619)[0;0m WARNING 12-09 20:59:31 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=51619)[0;0m INFO 12-09 20:59:31 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=51619)[0;0m WARNING 12-09 20:59:31 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 20:59:32 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:02:26 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='001bd107-5e21-488c-9353-1a9c82a33bb8', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:02:26 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:02:26 [model.py:1753] Using max model len 32768
INFO 12-09 21:02:26 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:02:26 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:31 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:32 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:46255 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:32 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:33 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:33 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:36 [default_loader.py:308] Loading weights took 1.96 seconds
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:36 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.695339 seconds
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:40 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:40 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:48 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.923 s
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:48 [monitor.py:34] torch.compile takes 10.64 s in total
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 001bd107-5e21-488c-9353-1a9c82a33bb8
[0;36m(EngineCore_DP0 pid=52613)[0;0m WARNING 12-09 21:02:50 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=52613)[0;0m WARNING 12-09 21:02:50 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:02:50 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:03:52 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:03:52 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.61 seconds
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:03:52 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 001bd107-5e21-488c-9353-1a9c82a33bb8
[0;36m(EngineCore_DP0 pid=52613)[0;0m WARNING 12-09 21:03:52 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=52613)[0;0m INFO 12-09 21:03:52 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=52613)[0;0m WARNING 12-09 21:03:52 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:03:52 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xsmall_uniq_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:07:12 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='87093033-fbad-4513-8e2f-36cc34fcff7d', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:07:13 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:07:13 [model.py:1753] Using max model len 32768
INFO 12-09 21:07:13 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:07:13 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:18 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:19 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:40681 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:19 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:19 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:20 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:22 [default_loader.py:308] Loading weights took 1.95 seconds
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:23 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.707573 seconds
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:27 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:27 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:34 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.940 s
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:35 [monitor.py:34] torch.compile takes 10.66 s in total
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:36 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 87093033-fbad-4513-8e2f-36cc34fcff7d
[0;36m(EngineCore_DP0 pid=53181)[0;0m WARNING 12-09 21:07:37 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=53181)[0;0m WARNING 12-09 21:07:37 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:37 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:42 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:42 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.25 seconds
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:42 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 87093033-fbad-4513-8e2f-36cc34fcff7d
[0;36m(EngineCore_DP0 pid=53181)[0;0m WARNING 12-09 21:07:42 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53181)[0;0m INFO 12-09 21:07:42 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=53181)[0;0m WARNING 12-09 21:07:42 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:07:43 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:10:31 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='b415f802-4382-4bb0-bb3a-f9ef88b02523', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:10:31 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:10:31 [model.py:1753] Using max model len 32768
INFO 12-09 21:10:31 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:10:31 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:36 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:37 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:40233 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:37 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:37 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:38 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:40 [default_loader.py:308] Loading weights took 1.95 seconds
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:41 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.684146 seconds
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:45 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:45 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:52 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.972 s
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:53 [monitor.py:34] torch.compile takes 10.69 s in total
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:54 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: b415f802-4382-4bb0-bb3a-f9ef88b02523
[0;36m(EngineCore_DP0 pid=53515)[0;0m WARNING 12-09 21:10:55 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=53515)[0;0m WARNING 12-09 21:10:55 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:10:55 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:11:57 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:11:57 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.78 seconds
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:11:57 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: b415f802-4382-4bb0-bb3a-f9ef88b02523
[0;36m(EngineCore_DP0 pid=53515)[0;0m WARNING 12-09 21:11:57 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53515)[0;0m INFO 12-09 21:11:57 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=53515)[0;0m WARNING 12-09 21:11:57 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:11:57 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_small_uniq_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:15:17 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='10a3a489-27d4-4e1d-9cef-87a1d0a612dc', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:15:18 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:15:18 [model.py:1753] Using max model len 32768
INFO 12-09 21:15:18 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:15:18 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:23 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:24 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:38927 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:24 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:24 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:25 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:27 [default_loader.py:308] Loading weights took 2.00 seconds
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:28 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.742328 seconds
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:32 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:32 [backends.py:715] Dynamo bytecode transform time: 3.81 s
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:40 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.944 s
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:40 [monitor.py:34] torch.compile takes 10.76 s in total
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 10a3a489-27d4-4e1d-9cef-87a1d0a612dc
[0;36m(EngineCore_DP0 pid=53880)[0;0m WARNING 12-09 21:15:42 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=53880)[0;0m WARNING 12-09 21:15:42 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:42 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:47 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:47 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.42 seconds
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:48 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 10a3a489-27d4-4e1d-9cef-87a1d0a612dc
[0;36m(EngineCore_DP0 pid=53880)[0;0m WARNING 12-09 21:15:48 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=53880)[0;0m INFO 12-09 21:15:48 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=53880)[0;0m WARNING 12-09 21:15:48 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:15:48 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:19:27 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='95629a50-793c-4a9e-946b-916782693188', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:19:27 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:19:27 [model.py:1753] Using max model len 32768
INFO 12-09 21:19:27 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:19:27 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:32 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:33 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:45027 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:33 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:33 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:34 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:36 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:37 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.695384 seconds
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:41 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:41 [backends.py:715] Dynamo bytecode transform time: 3.66 s
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:48 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.961 s
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:49 [monitor.py:34] torch.compile takes 10.62 s in total
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:50 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 95629a50-793c-4a9e-946b-916782693188
[0;36m(EngineCore_DP0 pid=54213)[0;0m WARNING 12-09 21:19:51 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=54213)[0;0m WARNING 12-09 21:19:51 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:19:51 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:20:52 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:20:52 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.46 seconds
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:20:53 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 95629a50-793c-4a9e-946b-916782693188
[0;36m(EngineCore_DP0 pid=54213)[0;0m WARNING 12-09 21:20:53 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54213)[0;0m INFO 12-09 21:20:53 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=54213)[0;0m WARNING 12-09 21:20:53 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:20:53 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_small_uniq_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:24:10 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='fd5211b1-7e7f-46fd-b69c-087236a23ce0', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:24:10 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:24:10 [model.py:1753] Using max model len 32768
INFO 12-09 21:24:10 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:24:10 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:16 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:16 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:44865 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:16 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:17 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:17 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:20 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:21 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.978610 seconds
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:24 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:24 [backends.py:715] Dynamo bytecode transform time: 3.75 s
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:32 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.047 s
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:33 [monitor.py:34] torch.compile takes 10.80 s in total
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:34 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: fd5211b1-7e7f-46fd-b69c-087236a23ce0
[0;36m(EngineCore_DP0 pid=54657)[0;0m WARNING 12-09 21:24:35 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=54657)[0;0m WARNING 12-09 21:24:35 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:35 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:40 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:40 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.48 seconds
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:40 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: fd5211b1-7e7f-46fd-b69c-087236a23ce0
[0;36m(EngineCore_DP0 pid=54657)[0;0m WARNING 12-09 21:24:40 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54657)[0;0m INFO 12-09 21:24:40 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=54657)[0;0m WARNING 12-09 21:24:40 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:24:41 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:27:44 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='5565b4d2-54f5-4b7f-88fe-e1d55d806fe4', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:27:44 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:27:44 [model.py:1753] Using max model len 32768
INFO 12-09 21:27:44 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:27:44 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:49 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:50 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:40639 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:50 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:51 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:51 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:54 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:54 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.712769 seconds
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:58 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:27:58 [backends.py:715] Dynamo bytecode transform time: 3.68 s
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:05 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.896 s
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:06 [monitor.py:34] torch.compile takes 10.58 s in total
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:07 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 5565b4d2-54f5-4b7f-88fe-e1d55d806fe4
[0;36m(EngineCore_DP0 pid=54980)[0;0m WARNING 12-09 21:28:08 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=54980)[0;0m WARNING 12-09 21:28:08 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:28:08 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:29:10 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:29:10 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.84 seconds
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:29:10 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 5565b4d2-54f5-4b7f-88fe-e1d55d806fe4
[0;36m(EngineCore_DP0 pid=54980)[0;0m WARNING 12-09 21:29:10 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=54980)[0;0m INFO 12-09 21:29:10 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=54980)[0;0m WARNING 12-09 21:29:10 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:29:11 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_medium_uniq_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:32:30 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='dc295b0a-f50d-43d2-803b-a688bfe79736', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:32:30 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:32:30 [model.py:1753] Using max model len 32768
INFO 12-09 21:32:30 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:32:30 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:36 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:36 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:47039 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:36 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:37 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:37 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:40 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:40 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.702822 seconds
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:44 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:44 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:52 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.996 s
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:53 [monitor.py:34] torch.compile takes 10.72 s in total
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: dc295b0a-f50d-43d2-803b-a688bfe79736
[0;36m(EngineCore_DP0 pid=55378)[0;0m WARNING 12-09 21:32:54 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=55378)[0;0m WARNING 12-09 21:32:54 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:32:54 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:33:00 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:33:00 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.40 seconds
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:33:00 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: dc295b0a-f50d-43d2-803b-a688bfe79736
[0;36m(EngineCore_DP0 pid=55378)[0;0m WARNING 12-09 21:33:00 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=55378)[0;0m INFO 12-09 21:33:00 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=55378)[0;0m WARNING 12-09 21:33:00 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:33:01 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:36:15 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='e5c46341-b7dd-493c-b1a7-5c7094d6b13b', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:36:16 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:36:16 [model.py:1753] Using max model len 32768
INFO 12-09 21:36:16 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:36:16 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:21 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:22 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:33759 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:22 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:22 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:23 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:37 [default_loader.py:308] Loading weights took 1.96 seconds
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:38 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 14.845724 seconds
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:42 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:42 [backends.py:715] Dynamo bytecode transform time: 3.69 s
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:49 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.928 s
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:50 [monitor.py:34] torch.compile takes 10.62 s in total
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: e5c46341-b7dd-493c-b1a7-5c7094d6b13b
[0;36m(EngineCore_DP0 pid=55741)[0;0m WARNING 12-09 21:36:51 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=55741)[0;0m WARNING 12-09 21:36:51 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:36:51 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:37:53 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:37:53 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.56 seconds
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:37:54 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: e5c46341-b7dd-493c-b1a7-5c7094d6b13b
[0;36m(EngineCore_DP0 pid=55741)[0;0m WARNING 12-09 21:37:54 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=55741)[0;0m INFO 12-09 21:37:54 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=55741)[0;0m WARNING 12-09 21:37:54 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:37:54 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_medium_uniq_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:41:14 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='2c0ad159-fd1e-4d4d-a571-c499df499df5', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:41:15 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:41:15 [model.py:1753] Using max model len 32768
INFO 12-09 21:41:15 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:41:15 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:20 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:21 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:48207 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:21 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:22 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:22 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:25 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:25 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.880219 seconds
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:29 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:29 [backends.py:715] Dynamo bytecode transform time: 3.79 s
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:36 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.569 s
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:37 [monitor.py:34] torch.compile takes 10.36 s in total
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:38 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 2c0ad159-fd1e-4d4d-a571-c499df499df5
[0;36m(EngineCore_DP0 pid=56100)[0;0m WARNING 12-09 21:41:39 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=56100)[0;0m WARNING 12-09 21:41:39 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:39 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:44 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:44 [core.py:253] init engine (profile, create kv cache, warmup model) took 18.95 seconds
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:44 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 2c0ad159-fd1e-4d4d-a571-c499df499df5
[0;36m(EngineCore_DP0 pid=56100)[0;0m WARNING 12-09 21:41:44 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56100)[0;0m INFO 12-09 21:41:44 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=56100)[0;0m WARNING 12-09 21:41:44 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:41:45 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:45:10 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='5e4c22d9-f153-4383-b45c-88b5d1353530', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:45:11 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:45:11 [model.py:1753] Using max model len 32768
INFO 12-09 21:45:11 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:45:11 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:16 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:17 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:51165 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:17 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:17 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:18 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:20 [default_loader.py:308] Loading weights took 1.94 seconds
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:21 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.760834 seconds
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:25 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:25 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:32 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.007 s
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:33 [monitor.py:34] torch.compile takes 10.72 s in total
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 5e4c22d9-f153-4383-b45c-88b5d1353530
[0;36m(EngineCore_DP0 pid=56425)[0;0m WARNING 12-09 21:45:34 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=56425)[0;0m WARNING 12-09 21:45:34 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:45:34 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:46:36 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:46:36 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.70 seconds
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:46:37 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 5e4c22d9-f153-4383-b45c-88b5d1353530
[0;36m(EngineCore_DP0 pid=56425)[0;0m WARNING 12-09 21:46:37 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56425)[0;0m INFO 12-09 21:46:37 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=56425)[0;0m WARNING 12-09 21:46:37 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:46:37 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_large_uniq_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:50:07 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='27e39175-0c6b-4047-a452-9034f50629d5', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:50:07 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:50:07 [model.py:1753] Using max model len 32768
INFO 12-09 21:50:07 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:50:07 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:13 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:13 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:47819 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:13 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:14 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:14 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:17 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:17 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.739405 seconds
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:21 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:21 [backends.py:715] Dynamo bytecode transform time: 3.73 s
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:29 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.982 s
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:30 [monitor.py:34] torch.compile takes 10.71 s in total
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 27e39175-0c6b-4047-a452-9034f50629d5
[0;36m(EngineCore_DP0 pid=56761)[0;0m WARNING 12-09 21:50:31 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=56761)[0;0m WARNING 12-09 21:50:31 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:31 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:37 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:37 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.32 seconds
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:37 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 27e39175-0c6b-4047-a452-9034f50629d5
[0;36m(EngineCore_DP0 pid=56761)[0;0m WARNING 12-09 21:50:37 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=56761)[0;0m INFO 12-09 21:50:37 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=56761)[0;0m WARNING 12-09 21:50:37 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:50:37 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 21:54:16 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='8bb267f8-2046-4647-9e77-607937d64ae6', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:54:17 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:54:17 [model.py:1753] Using max model len 32768
INFO 12-09 21:54:17 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:54:17 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:22 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:23 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:35305 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:23 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:23 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:24 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:26 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:27 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.811446 seconds
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:31 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:31 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:38 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.915 s
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:39 [monitor.py:34] torch.compile takes 10.64 s in total
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 8bb267f8-2046-4647-9e77-607937d64ae6
[0;36m(EngineCore_DP0 pid=57151)[0;0m WARNING 12-09 21:54:40 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=57151)[0;0m WARNING 12-09 21:54:40 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:54:40 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:55:42 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:55:42 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.56 seconds
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:55:43 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 8bb267f8-2046-4647-9e77-607937d64ae6
[0;36m(EngineCore_DP0 pid=57151)[0;0m WARNING 12-09 21:55:43 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57151)[0;0m INFO 12-09 21:55:43 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=57151)[0;0m WARNING 12-09 21:55:43 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:55:43 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_large_uniq_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 21:58:56 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='85178a39-f583-4966-9e75-65d4c59df6eb', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 21:58:56 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 21:58:56 [model.py:1753] Using max model len 32768
INFO 12-09 21:58:56 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 21:58:56 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:01 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:02 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:47353 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:02 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:02 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:03 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:06 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:06 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.790051 seconds
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:10 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:10 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:18 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.069 s
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:19 [monitor.py:34] torch.compile takes 10.78 s in total
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 85178a39-f583-4966-9e75-65d4c59df6eb
[0;36m(EngineCore_DP0 pid=57484)[0;0m WARNING 12-09 21:59:20 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=57484)[0;0m WARNING 12-09 21:59:20 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:20 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:25 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:25 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.31 seconds
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:26 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 85178a39-f583-4966-9e75-65d4c59df6eb
[0;36m(EngineCore_DP0 pid=57484)[0;0m WARNING 12-09 21:59:26 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57484)[0;0m INFO 12-09 21:59:26 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=57484)[0;0m WARNING 12-09 21:59:26 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 21:59:26 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:02:15 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='451e006b-3e83-4f06-ae3e-306311296b18', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:02:16 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:02:16 [model.py:1753] Using max model len 32768
INFO 12-09 22:02:16 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:02:16 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:21 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:22 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:37235 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:22 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:22 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:22 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:25 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:25 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.759339 seconds
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:29 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:29 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:37 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.869 s
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:38 [monitor.py:34] torch.compile takes 10.58 s in total
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 451e006b-3e83-4f06-ae3e-306311296b18
[0;36m(EngineCore_DP0 pid=57796)[0;0m WARNING 12-09 22:02:39 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=57796)[0;0m WARNING 12-09 22:02:39 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:02:39 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:03:41 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:03:41 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.90 seconds
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:03:42 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 451e006b-3e83-4f06-ae3e-306311296b18
[0;36m(EngineCore_DP0 pid=57796)[0;0m WARNING 12-09 22:03:42 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=57796)[0;0m INFO 12-09 22:03:42 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=57796)[0;0m WARNING 12-09 22:03:42 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:03:42 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xlarge_uniq_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:07:09 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='2529a6f7-ece2-47f5-b10a-aeeef9653e1e', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:07:10 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:07:10 [model.py:1753] Using max model len 32768
INFO 12-09 22:07:10 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:07:10 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:15 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:16 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:34901 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:16 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:16 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:17 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:19 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:20 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.729341 seconds
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:24 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:24 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:32 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.061 s
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:33 [monitor.py:34] torch.compile takes 10.77 s in total
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 2529a6f7-ece2-47f5-b10a-aeeef9653e1e
[0;36m(EngineCore_DP0 pid=58139)[0;0m WARNING 12-09 22:07:34 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=58139)[0;0m WARNING 12-09 22:07:34 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:34 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:39 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:39 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.39 seconds
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:40 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 2529a6f7-ece2-47f5-b10a-aeeef9653e1e
[0;36m(EngineCore_DP0 pid=58139)[0;0m WARNING 12-09 22:07:40 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58139)[0;0m INFO 12-09 22:07:40 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=58139)[0;0m WARNING 12-09 22:07:40 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:07:40 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:11:00 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='92d8f374-7ab9-4519-ba36-ebe599a2fa77', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:11:01 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:11:01 [model.py:1753] Using max model len 32768
INFO 12-09 22:11:01 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:11:01 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:06 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:07 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:37831 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:07 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:07 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:08 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:10 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:11 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.805367 seconds
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:15 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:15 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:22 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.865 s
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:23 [monitor.py:34] torch.compile takes 10.58 s in total
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 92d8f374-7ab9-4519-ba36-ebe599a2fa77
[0;36m(EngineCore_DP0 pid=58452)[0;0m WARNING 12-09 22:11:24 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=58452)[0;0m WARNING 12-09 22:11:24 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:11:24 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:12:26 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:12:26 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.37 seconds
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:12:26 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 92d8f374-7ab9-4519-ba36-ebe599a2fa77
[0;36m(EngineCore_DP0 pid=58452)[0;0m WARNING 12-09 22:12:26 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58452)[0;0m INFO 12-09 22:12:26 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=58452)[0;0m WARNING 12-09 22:12:26 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:12:27 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xlarge_uniq_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:15:43 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='66bda5b4-9928-4a26-842f-299e40a7a1be', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:15:43 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:15:43 [model.py:1753] Using max model len 32768
INFO 12-09 22:15:43 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:15:43 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:48 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:49 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:40001 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:49 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:50 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:50 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:53 [default_loader.py:308] Loading weights took 1.99 seconds
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:53 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.700894 seconds
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:57 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:15:57 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:05 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.998 s
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:06 [monitor.py:34] torch.compile takes 10.71 s in total
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 66bda5b4-9928-4a26-842f-299e40a7a1be
[0;36m(EngineCore_DP0 pid=58792)[0;0m WARNING 12-09 22:16:07 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=58792)[0;0m WARNING 12-09 22:16:07 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:07 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:13 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:13 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.31 seconds
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:13 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 66bda5b4-9928-4a26-842f-299e40a7a1be
[0;36m(EngineCore_DP0 pid=58792)[0;0m WARNING 12-09 22:16:13 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=58792)[0;0m INFO 12-09 22:16:13 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=58792)[0;0m WARNING 12-09 22:16:13 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:16:14 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:19:30 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='0394f99d-4433-4b1f-86c6-ad8a2622848c', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:19:30 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:19:30 [model.py:1753] Using max model len 32768
INFO 12-09 22:19:30 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:19:30 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:35 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:36 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:38471 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:36 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:36 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:37 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:39 [default_loader.py:308] Loading weights took 1.96 seconds
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:40 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.789197 seconds
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:44 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:44 [backends.py:715] Dynamo bytecode transform time: 3.67 s
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:51 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.964 s
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:52 [monitor.py:34] torch.compile takes 10.64 s in total
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:53 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 0394f99d-4433-4b1f-86c6-ad8a2622848c
[0;36m(EngineCore_DP0 pid=59101)[0;0m WARNING 12-09 22:19:54 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=59101)[0;0m WARNING 12-09 22:19:54 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:19:54 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:20:56 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:20:56 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.74 seconds
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:20:56 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 0394f99d-4433-4b1f-86c6-ad8a2622848c
[0;36m(EngineCore_DP0 pid=59101)[0;0m WARNING 12-09 22:20:56 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=59101)[0;0m INFO 12-09 22:20:56 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=59101)[0;0m WARNING 12-09 22:20:56 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:20:57 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xsmall_shared_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:24:16 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='ca8e2a46-58d3-4bf0-b41e-8acd894993dd', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:24:16 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:24:16 [model.py:1753] Using max model len 32768
INFO 12-09 22:24:16 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:24:16 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:22 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:23 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:51365 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:23 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:23 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:23 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:26 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:26 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.680546 seconds
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:30 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:30 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:38 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.091 s
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:39 [monitor.py:34] torch.compile takes 10.81 s in total
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: ca8e2a46-58d3-4bf0-b41e-8acd894993dd
[0;36m(EngineCore_DP0 pid=59835)[0;0m WARNING 12-09 22:24:40 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=59835)[0;0m WARNING 12-09 22:24:40 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:40 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:41 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:46 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:46 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.34 seconds
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:46 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: ca8e2a46-58d3-4bf0-b41e-8acd894993dd
[0;36m(EngineCore_DP0 pid=59835)[0;0m WARNING 12-09 22:24:46 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=59835)[0;0m INFO 12-09 22:24:46 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=59835)[0;0m WARNING 12-09 22:24:46 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:24:47 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:26:01 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='7b77e57a-fd0d-412f-a25d-77810ed50929', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:26:01 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:26:01 [model.py:1753] Using max model len 32768
INFO 12-09 22:26:01 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:26:01 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:06 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:07 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:42001 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:07 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:08 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:08 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:10 [default_loader.py:308] Loading weights took 1.95 seconds
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:11 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.684068 seconds
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:15 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:15 [backends.py:715] Dynamo bytecode transform time: 3.70 s
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:22 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.903 s
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:23 [monitor.py:34] torch.compile takes 10.60 s in total
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:24 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 7b77e57a-fd0d-412f-a25d-77810ed50929
[0;36m(EngineCore_DP0 pid=60137)[0;0m WARNING 12-09 22:26:25 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=60137)[0;0m WARNING 12-09 22:26:25 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:26:25 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:27:26 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:27:26 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.52 seconds
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:27:27 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 7b77e57a-fd0d-412f-a25d-77810ed50929
[0;36m(EngineCore_DP0 pid=60137)[0;0m WARNING 12-09 22:27:27 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60137)[0;0m INFO 12-09 22:27:27 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=60137)[0;0m WARNING 12-09 22:27:27 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:27:27 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xsmall_shared_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:29:10 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='7c6937c0-4637-4a35-a37b-459c1c3b6787', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:29:11 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:29:11 [model.py:1753] Using max model len 32768
INFO 12-09 22:29:11 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:29:11 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:16 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:17 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:53951 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:17 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:17 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:18 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:20 [default_loader.py:308] Loading weights took 1.99 seconds
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:21 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.779280 seconds
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:25 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:25 [backends.py:715] Dynamo bytecode transform time: 3.75 s
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:32 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.915 s
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:33 [monitor.py:34] torch.compile takes 10.66 s in total
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 7c6937c0-4637-4a35-a37b-459c1c3b6787
[0;36m(EngineCore_DP0 pid=60488)[0;0m WARNING 12-09 22:29:34 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=60488)[0;0m WARNING 12-09 22:29:34 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:34 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:35 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:40 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:40 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.09 seconds
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:40 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 7c6937c0-4637-4a35-a37b-459c1c3b6787
[0;36m(EngineCore_DP0 pid=60488)[0;0m WARNING 12-09 22:29:40 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60488)[0;0m INFO 12-09 22:29:40 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=60488)[0;0m WARNING 12-09 22:29:40 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:29:41 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:31:02 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='09478971-4fa5-4c75-bacc-b6831d3129c9', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:31:02 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:31:02 [model.py:1753] Using max model len 32768
INFO 12-09 22:31:02 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:31:02 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:08 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:08 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:33345 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:08 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:09 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:09 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:12 [default_loader.py:308] Loading weights took 1.94 seconds
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:12 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.662167 seconds
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:16 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:16 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:24 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.989 s
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:24 [monitor.py:34] torch.compile takes 10.71 s in total
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 09478971-4fa5-4c75-bacc-b6831d3129c9
[0;36m(EngineCore_DP0 pid=60787)[0;0m WARNING 12-09 22:31:26 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=60787)[0;0m WARNING 12-09 22:31:26 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:31:26 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:32:28 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:32:28 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.52 seconds
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:32:28 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 09478971-4fa5-4c75-bacc-b6831d3129c9
[0;36m(EngineCore_DP0 pid=60787)[0;0m WARNING 12-09 22:32:28 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=60787)[0;0m INFO 12-09 22:32:28 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=60787)[0;0m WARNING 12-09 22:32:28 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:32:28 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_small_shared_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:34:19 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='59cdc5a1-c894-49b9-a81f-4734c958e242', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:34:19 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:34:19 [model.py:1753] Using max model len 32768
INFO 12-09 22:34:19 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:34:19 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:25 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:26 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:35223 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:26 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:26 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:27 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:29 [default_loader.py:308] Loading weights took 1.96 seconds
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:29 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.697391 seconds
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:33 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:33 [backends.py:715] Dynamo bytecode transform time: 3.77 s
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:41 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.971 s
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:42 [monitor.py:34] torch.compile takes 10.74 s in total
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 59cdc5a1-c894-49b9-a81f-4734c958e242
[0;36m(EngineCore_DP0 pid=61139)[0;0m WARNING 12-09 22:34:43 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=61139)[0;0m WARNING 12-09 22:34:43 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:43 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:44 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:49 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:49 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.18 seconds
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:49 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 59cdc5a1-c894-49b9-a81f-4734c958e242
[0;36m(EngineCore_DP0 pid=61139)[0;0m WARNING 12-09 22:34:49 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61139)[0;0m INFO 12-09 22:34:49 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=61139)[0;0m WARNING 12-09 22:34:49 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:34:49 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:35:52 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='b65a21d0-080c-4d37-8df9-4c6db25bb290', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:35:53 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:35:53 [model.py:1753] Using max model len 32768
INFO 12-09 22:35:53 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:35:53 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:35:58 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:35:59 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:36863 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:35:59 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:35:59 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:35:59 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:02 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:03 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.982699 seconds
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:07 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:07 [backends.py:715] Dynamo bytecode transform time: 3.70 s
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:14 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.484 s
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:14 [monitor.py:34] torch.compile takes 10.19 s in total
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: b65a21d0-080c-4d37-8df9-4c6db25bb290
[0;36m(EngineCore_DP0 pid=61438)[0;0m WARNING 12-09 22:36:16 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=61438)[0;0m WARNING 12-09 22:36:16 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:36:16 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:37:18 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:37:18 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.14 seconds
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:37:18 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: b65a21d0-080c-4d37-8df9-4c6db25bb290
[0;36m(EngineCore_DP0 pid=61438)[0;0m WARNING 12-09 22:37:18 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61438)[0;0m INFO 12-09 22:37:18 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=61438)[0;0m WARNING 12-09 22:37:18 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:37:19 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_small_shared_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:39:03 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='77691b9b-5b08-4721-baa5-fcdebe5d33d7', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:39:03 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:39:03 [model.py:1753] Using max model len 32768
INFO 12-09 22:39:03 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:39:03 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:09 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:09 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:35269 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:09 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:10 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:10 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:13 [default_loader.py:308] Loading weights took 1.95 seconds
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:13 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.722409 seconds
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:17 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:17 [backends.py:715] Dynamo bytecode transform time: 3.69 s
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:25 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.964 s
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:26 [monitor.py:34] torch.compile takes 10.66 s in total
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 77691b9b-5b08-4721-baa5-fcdebe5d33d7
[0;36m(EngineCore_DP0 pid=61753)[0;0m WARNING 12-09 22:39:27 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=61753)[0;0m WARNING 12-09 22:39:27 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:27 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:32 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:32 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.11 seconds
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:33 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 77691b9b-5b08-4721-baa5-fcdebe5d33d7
[0;36m(EngineCore_DP0 pid=61753)[0;0m WARNING 12-09 22:39:33 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=61753)[0;0m INFO 12-09 22:39:33 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=61753)[0;0m WARNING 12-09 22:39:33 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:39:33 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:40:53 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='fa786e23-12c9-4197-bce3-7ccc1095a6c0', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:40:53 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:40:53 [model.py:1753] Using max model len 32768
INFO 12-09 22:40:53 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:40:53 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:40:59 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:40:59 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:33613 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:40:59 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:00 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:00 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:03 [default_loader.py:308] Loading weights took 1.96 seconds
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:03 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.675563 seconds
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:07 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:07 [backends.py:715] Dynamo bytecode transform time: 3.69 s
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:15 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.911 s
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:15 [monitor.py:34] torch.compile takes 10.60 s in total
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: fa786e23-12c9-4197-bce3-7ccc1095a6c0
[0;36m(EngineCore_DP0 pid=62050)[0;0m WARNING 12-09 22:41:17 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=62050)[0;0m WARNING 12-09 22:41:17 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:41:17 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:42:18 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:42:18 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.37 seconds
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:42:19 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: fa786e23-12c9-4197-bce3-7ccc1095a6c0
[0;36m(EngineCore_DP0 pid=62050)[0;0m WARNING 12-09 22:42:19 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62050)[0;0m INFO 12-09 22:42:19 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=62050)[0;0m WARNING 12-09 22:42:19 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:42:19 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_medium_shared_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:43:49 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='cf27d4e4-8cf2-4953-a54b-5c22a397b41d', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:43:49 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:43:49 [model.py:1753] Using max model len 32768
INFO 12-09 22:43:49 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:43:49 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:55 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:56 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:44561 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:56 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:56 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:56 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:59 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:43:59 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.706573 seconds
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:03 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:03 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:11 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.021 s
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:12 [monitor.py:34] torch.compile takes 10.74 s in total
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: cf27d4e4-8cf2-4953-a54b-5c22a397b41d
[0;36m(EngineCore_DP0 pid=62372)[0;0m WARNING 12-09 22:44:13 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=62372)[0;0m WARNING 12-09 22:44:13 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:13 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:19 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:19 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.28 seconds
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:19 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: cf27d4e4-8cf2-4953-a54b-5c22a397b41d
[0;36m(EngineCore_DP0 pid=62372)[0;0m WARNING 12-09 22:44:19 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62372)[0;0m INFO 12-09 22:44:19 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=62372)[0;0m WARNING 12-09 22:44:19 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:44:20 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:45:39 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='6f752a53-ae48-4e75-9e6c-5661aef2112a', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:45:39 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:45:39 [model.py:1753] Using max model len 32768
INFO 12-09 22:45:39 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:45:39 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:45 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:45 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:60045 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:45 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:46 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:46 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:49 [default_loader.py:308] Loading weights took 1.95 seconds
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:49 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.692829 seconds
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:53 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:45:53 [backends.py:715] Dynamo bytecode transform time: 3.66 s
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:01 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.980 s
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:01 [monitor.py:34] torch.compile takes 10.64 s in total
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 6f752a53-ae48-4e75-9e6c-5661aef2112a
[0;36m(EngineCore_DP0 pid=62669)[0;0m WARNING 12-09 22:46:03 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=62669)[0;0m WARNING 12-09 22:46:03 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:46:03 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:47:05 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:47:05 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.72 seconds
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:47:05 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 6f752a53-ae48-4e75-9e6c-5661aef2112a
[0;36m(EngineCore_DP0 pid=62669)[0;0m WARNING 12-09 22:47:05 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=62669)[0;0m INFO 12-09 22:47:05 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=62669)[0;0m WARNING 12-09 22:47:05 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:47:06 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_medium_shared_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:48:32 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='f66af853-467d-4e81-a7e4-08e6d0a1338b', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:48:32 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:48:32 [model.py:1753] Using max model len 32768
INFO 12-09 22:48:32 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:48:32 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:38 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:40 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:43745 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:40 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:40 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:41 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:43 [default_loader.py:308] Loading weights took 1.99 seconds
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:48:56 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.728875 seconds
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:49:00 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:49:00 [backends.py:715] Dynamo bytecode transform time: 3.76 s
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:49:08 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.126 s
[0;36m(EngineCore_DP0 pid=64742)[0;0m INFO 12-09 22:49:09 [monitor.py:34] torch.compile takes 10.89 s in total
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/engine/core.py", line 833, in run_engine_core
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/engine/core.py", line 609, in __init__
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     super().__init__(
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/engine/core.py", line 109, in __init__
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/engine/core.py", line 234, in _initialize_kv_caches
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     return self.collective_rpc("determine_available_memory")
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/serial_utils.py", line 479, in run_method
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     return func(*args, **kwargs)
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]   File "/home/azureuser/vllm/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]     assert self.init_snapshot.free_memory > free_gpu_memory, (
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=64742)[0;0m ERROR 12-09 22:49:10 [core.py:842] AssertionError: Error in memory profiling. Initial free memory 61.2198486328125 GiB, current free memory 78.16668701171875 GiB. This happens when other processes sharing the same container release GPU memory while vLLM is profiling during initialization. To fix this, ensure consistent GPU memory allocation or isolate vLLM in its own container.
Error running test for config mistral-7b-instruct-v0.1_medium_shared_arc: Engine core initialization failed. See root cause above. Failed core proc(s): {}
CPU offloading with 42,657 blocks
INFO 12-09 22:49:12 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='573cdd57-fac5-4684-a4e3-026c11421cdd', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:49:12 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:49:12 [model.py:1753] Using max model len 32768
INFO 12-09 22:49:12 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:49:12 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:18 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:19 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:42261 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:19 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:20 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:20 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:23 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:23 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.691781 seconds
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:27 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:27 [backends.py:715] Dynamo bytecode transform time: 3.75 s
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:35 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.111 s
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:36 [monitor.py:34] torch.compile takes 10.86 s in total
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 573cdd57-fac5-4684-a4e3-026c11421cdd
[0;36m(EngineCore_DP0 pid=65468)[0;0m WARNING 12-09 22:49:37 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=65468)[0;0m WARNING 12-09 22:49:37 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:49:37 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:50:39 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:50:39 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.97 seconds
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:50:39 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 573cdd57-fac5-4684-a4e3-026c11421cdd
[0;36m(EngineCore_DP0 pid=65468)[0;0m WARNING 12-09 22:50:39 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=65468)[0;0m INFO 12-09 22:50:39 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=65468)[0;0m WARNING 12-09 22:50:39 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:50:40 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_large_shared_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:52:12 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='e4635857-1fa7-4c44-b202-094f8c961fbb', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:52:13 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:52:13 [model.py:1753] Using max model len 32768
INFO 12-09 22:52:13 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:52:13 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:18 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:19 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:48607 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:19 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:19 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:20 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:22 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:23 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.680745 seconds
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:27 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:27 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:34 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.900 s
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:35 [monitor.py:34] torch.compile takes 10.61 s in total
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:36 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: e4635857-1fa7-4c44-b202-094f8c961fbb
[0;36m(EngineCore_DP0 pid=66295)[0;0m WARNING 12-09 22:52:37 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=66295)[0;0m WARNING 12-09 22:52:37 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:37 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:42 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:42 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.14 seconds
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:42 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: e4635857-1fa7-4c44-b202-094f8c961fbb
[0;36m(EngineCore_DP0 pid=66295)[0;0m WARNING 12-09 22:52:42 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=66295)[0;0m INFO 12-09 22:52:42 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=66295)[0;0m WARNING 12-09 22:52:42 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:52:43 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:54:00 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='0767a086-6c15-4a20-ada2-eb576fadf5eb', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:54:00 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:54:00 [model.py:1753] Using max model len 32768
INFO 12-09 22:54:00 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:54:00 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:06 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:07 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:45341 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:07 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:07 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:07 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:10 [default_loader.py:308] Loading weights took 1.99 seconds
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:10 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.724076 seconds
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:14 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:14 [backends.py:715] Dynamo bytecode transform time: 3.78 s
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:22 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.013 s
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:23 [monitor.py:34] torch.compile takes 10.80 s in total
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 0767a086-6c15-4a20-ada2-eb576fadf5eb
[0;36m(EngineCore_DP0 pid=66703)[0;0m WARNING 12-09 22:54:24 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=66703)[0;0m WARNING 12-09 22:54:24 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:54:24 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:55:26 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:55:26 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.97 seconds
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:55:27 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 0767a086-6c15-4a20-ada2-eb576fadf5eb
[0;36m(EngineCore_DP0 pid=66703)[0;0m WARNING 12-09 22:55:27 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=66703)[0;0m INFO 12-09 22:55:27 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=66703)[0;0m WARNING 12-09 22:55:27 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:55:27 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_large_shared_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 22:57:13 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='3705d491-0935-4deb-aa23-884a90f666f2', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:57:13 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:57:13 [model.py:1753] Using max model len 32768
INFO 12-09 22:57:13 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:57:13 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:19 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:20 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:51431 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:20 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:20 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:20 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:23 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:23 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.733461 seconds
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:27 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:27 [backends.py:715] Dynamo bytecode transform time: 3.73 s
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:35 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.555 s
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:35 [monitor.py:34] torch.compile takes 10.29 s in total
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 3705d491-0935-4deb-aa23-884a90f666f2
[0;36m(EngineCore_DP0 pid=67059)[0;0m WARNING 12-09 22:57:37 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=67059)[0;0m WARNING 12-09 22:57:37 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:37 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:42 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:42 [core.py:253] init engine (profile, create kv cache, warmup model) took 18.99 seconds
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:43 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 3705d491-0935-4deb-aa23-884a90f666f2
[0;36m(EngineCore_DP0 pid=67059)[0;0m WARNING 12-09 22:57:43 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67059)[0;0m INFO 12-09 22:57:43 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=67059)[0;0m WARNING 12-09 22:57:43 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 22:57:43 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 22:59:08 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='d3f54a3d-d56c-4c12-8be5-0395c1b445e5', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 22:59:09 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 22:59:09 [model.py:1753] Using max model len 32768
INFO 12-09 22:59:09 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 22:59:09 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:14 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:15 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:40031 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:15 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:15 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:16 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:18 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:19 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.697537 seconds
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:23 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:23 [backends.py:715] Dynamo bytecode transform time: 3.72 s
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:30 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.884 s
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:31 [monitor.py:34] torch.compile takes 10.61 s in total
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: d3f54a3d-d56c-4c12-8be5-0395c1b445e5
[0;36m(EngineCore_DP0 pid=67362)[0;0m WARNING 12-09 22:59:32 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=67362)[0;0m WARNING 12-09 22:59:32 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 22:59:32 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 23:00:34 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 23:00:34 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.53 seconds
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 23:00:34 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: d3f54a3d-d56c-4c12-8be5-0395c1b445e5
[0;36m(EngineCore_DP0 pid=67362)[0;0m WARNING 12-09 23:00:34 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67362)[0;0m INFO 12-09 23:00:34 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=67362)[0;0m WARNING 12-09 23:00:34 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 23:00:35 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xlarge_shared_lru (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 23:02:22 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='aad78fcf-dc20-482d-9084-a26e2688e231', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 23:02:22 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 23:02:22 [model.py:1753] Using max model len 32768
INFO 12-09 23:02:22 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 23:02:22 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:28 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:28 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:40963 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:28 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:29 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:29 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:32 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:32 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.728856 seconds
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:36 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:36 [backends.py:715] Dynamo bytecode transform time: 3.71 s
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:44 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.078 s
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:45 [monitor.py:34] torch.compile takes 10.79 s in total
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: aad78fcf-dc20-482d-9084-a26e2688e231
[0;36m(EngineCore_DP0 pid=67697)[0;0m WARNING 12-09 23:02:46 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=67697)[0;0m WARNING 12-09 23:02:46 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:46 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:47 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:52 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:52 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.55 seconds
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:52 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: aad78fcf-dc20-482d-9084-a26e2688e231
[0;36m(EngineCore_DP0 pid=67697)[0;0m WARNING 12-09 23:02:52 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=67697)[0;0m INFO 12-09 23:02:52 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=67697)[0;0m WARNING 12-09 23:02:52 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 23:02:53 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 23:03:58 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='9f8c620e-3cd1-4d75-a42b-a5c8aa185190', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'lru'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 23:03:59 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 23:03:59 [model.py:1753] Using max model len 32768
INFO 12-09 23:03:59 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 23:03:59 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:04 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:05 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:49761 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:05 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:05 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:06 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:08 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:09 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.724306 seconds
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:13 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:13 [backends.py:715] Dynamo bytecode transform time: 3.69 s
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:20 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.527 s
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:21 [monitor.py:34] torch.compile takes 10.22 s in total
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 9f8c620e-3cd1-4d75-a42b-a5c8aa185190
[0;36m(EngineCore_DP0 pid=68005)[0;0m WARNING 12-09 23:04:22 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=68005)[0;0m WARNING 12-09 23:04:22 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:04:22 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:05:24 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:05:24 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.20 seconds
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:05:24 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 9f8c620e-3cd1-4d75-a42b-a5c8aa185190
[0;36m(EngineCore_DP0 pid=68005)[0;0m WARNING 12-09 23:05:24 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68005)[0;0m INFO 12-09 23:05:24 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=68005)[0;0m WARNING 12-09 23:05:24 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 23:05:25 [llm.py:346] Supported tasks: ['generate']

============================================================
Running tests for: mistral-7b-instruct-v0.1_xlarge_shared_arc (1000 prompts)
============================================================
GPU offloading with 42,657 blocks
INFO 12-09 23:07:15 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='4cf31e15-5e95-4d11-8e77-334fb61d371c', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'GPUOffloadingSpec', 'num_gpu_blocks': 42657, 'dest_gpu_id': 1, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 23:07:16 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 23:07:16 [model.py:1753] Using max model len 32768
INFO 12-09 23:07:16 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 23:07:16 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:21 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:22 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:58641 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:22 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:22 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:23 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:25 [default_loader.py:308] Loading weights took 1.97 seconds
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:26 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.725929 seconds
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:30 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:30 [backends.py:715] Dynamo bytecode transform time: 3.80 s
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:38 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.995 s
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:38 [monitor.py:34] torch.compile takes 10.80 s in total
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [gpu_worker.py:348] Available KV cache memory: 12.58 GiB
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [kv_cache_utils.py:1286] GPU KV cache size: 103,008 tokens
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.03x
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 4cf31e15-5e95-4d11-8e77-334fb61d371c
[0;36m(EngineCore_DP0 pid=68330)[0;0m WARNING 12-09 23:07:40 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=68330)[0;0m WARNING 12-09 23:07:40 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6438, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:40 [gpu_gpu.py:99] Allocating 1 secondary GPU tensors on cuda:1...
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:45 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:45 [core.py:253] init engine (profile, create kv cache, warmup model) took 19.63 seconds
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:46 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 4cf31e15-5e95-4d11-8e77-334fb61d371c
[0;36m(EngineCore_DP0 pid=68330)[0;0m WARNING 12-09 23:07:46 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68330)[0;0m INFO 12-09 23:07:46 [factory.py:49] Creating offloading spec with name: GPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=68330)[0;0m WARNING 12-09 23:07:46 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 23:07:46 [llm.py:346] Supported tasks: ['generate']
CPU offloading with 42,657 blocks
INFO 12-09 23:08:57 [utils.py:253] non-default args: {'seed': None, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='OffloadingConnector', engine_id='9fbbef64-1482-4a1b-8846-581aced33bb3', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={'spec_name': 'CPUOffloadingSpec', 'num_cpu_blocks': 42657, 'eviction_policy': 'arc'}, kv_connector_module_path=None, enable_permute_local_kv=False), 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-09 23:08:58 [model.py:643] Resolved architecture: MistralForCausalLM
INFO 12-09 23:08:58 [model.py:1753] Using max model len 32768
INFO 12-09 23:08:58 [scheduler.py:207] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 12-09 23:08:58 [vllm.py:729] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:03 [core.py:93] Initializing a V1 LLM engine (v0.1.dev11691+g9f95bf33a.d20251209) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:04 [parallel_state.py:1217] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.0.4:45951 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:04 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:04 [gpu_model_runner.py:3378] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:05 [cuda.py:416] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:07 [default_loader.py:308] Loading weights took 1.98 seconds
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:08 [gpu_model_runner.py:3460] Model loading took 13.4967 GiB memory and 2.722177 seconds
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:11 [backends.py:655] Using cache directory: /home/azureuser/.cache/vllm/torch_compile_cache/935b835b79/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:11 [backends.py:715] Dynamo bytecode transform time: 3.67 s
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:19 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.996 s
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:20 [monitor.py:34] torch.compile takes 10.67 s in total
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [gpu_worker.py:348] Available KV cache memory: 12.57 GiB
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [kv_cache_utils.py:1286] GPU KV cache size: 102,992 tokens
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [kv_cache_utils.py:1291] Maximum concurrency for 32,768 tokens per request: 5.02x
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 9fbbef64-1482-4a1b-8846-581aced33bb3
[0;36m(EngineCore_DP0 pid=68634)[0;0m WARNING 12-09 23:09:21 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=68634)[0;0m WARNING 12-09 23:09:21 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [kv_connector_model_runner_mixin.py:297] Allocating a cross layer KV cache of shape (6437, 32, 2, 16, 8, 128)
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:09:21 [cpu_gpu.py:85] Allocating 1 CPU tensors...
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:10:23 [gpu_model_runner.py:4381] Graph capturing finished in 5 secs, took -1.65 GiB
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:10:23 [core.py:253] init engine (profile, create kv cache, warmup model) took 75.93 seconds
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:10:24 [factory.py:64] Creating v1 connector with name: OffloadingConnector and engine_id: 9fbbef64-1482-4a1b-8846-581aced33bb3
[0;36m(EngineCore_DP0 pid=68634)[0;0m WARNING 12-09 23:10:24 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=68634)[0;0m INFO 12-09 23:10:24 [factory.py:49] Creating offloading spec with name: CPUOffloadingSpec
[0;36m(EngineCore_DP0 pid=68634)[0;0m WARNING 12-09 23:10:24 [spec.py:24] Initializing OffloadingSpec. This API is experimental and subject to change in the future as we iterate the design.
INFO 12-09 23:10:24 [llm.py:346] Supported tasks: ['generate']

Results for policy=lru:
Model                              Config Size   Prompts Policy MaxSeq AvgIn GPU_s  CPU_s  GPU_tok_s CPU_tok_s Compute_used_GB Dest_used_GB Offload_kv_capacity_GB GPU_mem_util Speedup_s Speedup_pct Offload Reload Trials
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
mistralai/Mistral-7B-Instruct-v0.1        xsmall uniq    lru    50     105   174.08 169.88 35069.33  35937.65  31.619219456    90.733740032 89.458                 0.3          -4.21     -2.5        52442   3      1     
mistralai/Mistral-7B-Instruct-v0.1        xsmall shared  lru    50     80    74.14  73.13  82010.94  83139.63  31.68632832     90.733740032 89.458                 0.3          -1.01     -1.4        29325   124    1     
mistralai/Mistral-7B-Instruct-v0.1        small  uniq    lru    100    105   218.62 166.72 27925.16  36617.26  31.68632832     90.733740032 89.458                 0.3          -51.9     -31.1       58260   2      1     
mistralai/Mistral-7B-Instruct-v0.1        small  shared  lru    100    80    62.85  74.55  96732.58  81560.84  31.68632832     90.733740032 89.458                 0.3          11.69     15.7        27518   119    1     
mistralai/Mistral-7B-Instruct-v0.1        medium uniq    lru    200    105   194.89 170.62 31325.1   35781.09  31.68632832     90.733740032 89.458                 0.3          -24.27    -14.2       54609   14     1     
mistralai/Mistral-7B-Instruct-v0.1        medium shared  lru    200    80    79.56  80.25  76418.26  75767.86  31.68632832     90.733740032 89.458                 0.3          0.68      0.9         28622   115    1     
mistralai/Mistral-7B-Instruct-v0.1        large  uniq    lru    300    105   218.93 162.58 27885.69  37550.83  31.68632832     90.733740032 89.458                 0.3          -56.35    -34.7       58260   2      1     
mistralai/Mistral-7B-Instruct-v0.1        large  shared  lru    300    80    77.06  75.99  78901.86  80011.87  31.68632832     90.733740032 89.458                 0.3          -1.07     -1.4        29369   116    1     
mistralai/Mistral-7B-Instruct-v0.1        xlarge uniq    lru    500    105   200.24 166.06 30488.7   36763.91  31.68632832     90.733740032 89.458                 0.3          -34.18    -20.6       55179   7      1     
mistralai/Mistral-7B-Instruct-v0.1        xlarge shared  lru    500    80    65.7   80.85  92547.99  75197.19  31.68632832     90.733740032 89.458                 0.3          15.16     18.7        28125   131    1     

Results for policy=arc:
Model                              Config Size   Prompts Policy MaxSeq AvgIn GPU_s  CPU_s  GPU_tok_s CPU_tok_s Compute_used_GB Dest_used_GB Offload_kv_capacity_GB GPU_mem_util Speedup_s Speedup_pct Offload Reload Trials
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
mistralai/Mistral-7B-Instruct-v0.1        xsmall uniq    arc    50     105   167.71 170.14 36401.26  35881.92  31.68632832     90.733740032 89.458                 0.3          2.43      1.4         49961   4      1     
mistralai/Mistral-7B-Instruct-v0.1        xsmall shared  arc    50     80    81.43  80.82  74661.01  75232.05  31.68632832     90.733740032 89.458                 0.3          -0.62     -0.8        29422   156    1     
mistralai/Mistral-7B-Instruct-v0.1        small  uniq    arc    100    105   182.75 169.35 33405.94  36049.52  31.68632832     90.733740032 89.458                 0.3          -13.4     -7.9        52727   3      1     
mistralai/Mistral-7B-Instruct-v0.1        small  shared  arc    100    80    79.79  60.01  76202.53  101314.35 31.68632832     90.733740032 89.458                 0.3          -19.78    -33.0       28622   115    1     
mistralai/Mistral-7B-Instruct-v0.1        medium uniq    arc    200    105   205.31 179.87 29735.74  33942.05  31.68632832     90.733740032 89.458                 0.3          -25.44    -14.1       57412   9      1     
mistralai/Mistral-7B-Instruct-v0.1        medium shared  arc    200    80    None   63.15  None      96284.81  None            None         89.458                 0.3          None      None        N/A     N/A    1     
mistralai/Mistral-7B-Instruct-v0.1        large  uniq    arc    300    105   169.11 177.41 36099.88  34412.46  31.68632832     90.733740032 89.458                 0.3          8.29      4.7         50461   3      1     
mistralai/Mistral-7B-Instruct-v0.1        large  shared  arc    300    80    85.14  77.11  71413.37  78852.7   31.68632832     90.733740032 89.458                 0.3          -8.03     -10.4       30643   107    1     
mistralai/Mistral-7B-Instruct-v0.1        xlarge uniq    arc    500    105   196.15 169.56 31124.33  36005.31  31.68632832     90.733740032 89.458                 0.3          -26.59    -15.7       53503   8      1     
mistralai/Mistral-7B-Instruct-v0.1        xlarge shared  arc    500    80    70.87  81.74  85793.77  74378.76  31.68632832     90.733740032 89.458                 0.3          10.88     13.3        28764   147    1     

################################################################################
SUMMARY
################################################################################
Total configs tested: 20
Models: ['mistralai/Mistral-7B-Instruct-v0.1']
Size tiers: ['xsmall', 'small', 'medium', 'large', 'xlarge']
Prompt sets: ['unique', 'shared_prefix']
Eviction policies: ['lru', 'arc']
Trials per config: 1
Max tokens: 6000
################################################################################
Per-trial rows streamed to /home/azureuser/vllm/nikhil-tests/results/20251209-205814.csv
ERROR 12-09 23:11:51 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
ERROR 12-09 23:12:16 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
